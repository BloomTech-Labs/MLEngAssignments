{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_Engineer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zagwj7Qg_l4c"
      },
      "source": [
        "# Machine Learning Engineer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzD3kWv71N3T"
      },
      "source": [
        "## Intorduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20oCzfEwoPCA"
      },
      "source": [
        "### Random Monsters: MonsterLab & Fortuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxlFiKRaG74O"
      },
      "source": [
        "Fortuna is a random value toolkit by Robert Sharp. If you would like to know more, here's the [Fortuna Documentation](https://pypi.org/project/Fortuna/). Unfortunately, Fortuna is currently incompatible with Windows. As such, it is recommended to run this notebook with Colab or Jupyter on WSL. Fortuna is 100% compatible with all *nix systems including MacOS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCo6nZ9rnv_z"
      },
      "source": [
        "!pip install MonsterLab --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDnBfBzsnno7"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "from time import sleep\n",
        "from MonsterLab import Monster"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBrioc8U1W-a"
      },
      "source": [
        "Before we can do machine learning we need some data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI4YHxMaF9L9"
      },
      "source": [
        "A Random Monster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyfhF9f0o-Xi"
      },
      "source": [
        "Monster()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaPjg2EGisM9"
      },
      "source": [
        "Generate Mock Monster Data.\n",
        "\n",
        "5000 should make for a good model, but play with it, see what you can find with different values for the `number` variable below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0SIe9jOWmKW"
      },
      "source": [
        "number = 5000\n",
        "\n",
        "df = pd.DataFrame(Monster().to_dict() for _ in range(number))\n",
        "\n",
        "df.to_csv(\"monsters.csv\", index=False)\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtZpvTPpFCAK"
      },
      "source": [
        "## Assignment 1 - ML Model Interface Intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrbTGOy8PfeX"
      },
      "source": [
        "### Abstraction, Encapsulation, Polymophism\n",
        "\n",
        "Below is one example of an abstraction that encasulates an ML model and extends some customization points. Here we'll use a class interface, but functions can work too.\n",
        "\n",
        "You can parameterize every aspect of the model by adding arguments to the init method. Be mindfull, you don't want to over-do it here. Keep your calling signature simple and usable. Provide good defaults and well named arguments, and your users will enjoy using your code. Make it super complicated and they may as well just use Scikit themselves.\n",
        "\n",
        "A good interface should always encapsulate the core logic in such a way that the rest of the app is totally unaware of how it works, but can still interact with the core logic in a general way. One might say that a good interface is always more abstract than the core logic it encapsulates. At this higher abstraction level it becomes easier to replace our core logic without disrupting parallel development on other parts of the app. And now a word from our sponsor, Polymorphism.\n",
        "\n",
        "One hypothetical example of Polymorphism is if we designed more than one ML model, possibly with two different ML libraries. Then gave them compatible interfaces. This gives us the ability to trade one model library for another without rewriting the whole app. We could do that at any time during development without disrupting anything.\n",
        "\n",
        "A Polymorphic system is built to be modular from the start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ozrbphkc_wXx"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULYXPaDJFs0o"
      },
      "source": [
        "### Model Interface: RandomForest Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGJljmKI_c1m"
      },
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self, target: pd.Series, features: pd.DataFrame):\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
        "            features,\n",
        "            target,\n",
        "            test_size=0.20,\n",
        "            stratify=target,\n",
        "            random_state=42,\n",
        "        )\n",
        "        self.model = RandomForestClassifier(\n",
        "            n_jobs=-1,\n",
        "            random_state=42,\n",
        "        )\n",
        "        self.model.fit(self.X_train, self.y_train)\n",
        "        self.train_score = self.model.score(self.X_train, self.y_train) - 0.0001\n",
        "        self.test_score = self.model.score(self.X_test, self.y_test)\n",
        "    \n",
        "    def __call__(self, pred_input):\n",
        "        prediction, *_ = self.model.predict([pred_input])\n",
        "        probability, *_ = self.model.predict_proba([pred_input])\n",
        "        confidence = max(probability) * self.test_score\n",
        "        return prediction, confidence\n",
        "    \n",
        "    def __repr__(self):\n",
        "        train = f\"Training Score: {100*self.train_score:.2f}%\"\n",
        "        test = f\"Validation Score: {100*self.test_score:.2f}%\"\n",
        "        return f\"Model(target, features)\\n{train}\\n{test}\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqXXESJWs7wT"
      },
      "source": [
        "Create a model interface for your favorite Scikit model by completing the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdFgvwSUtCFO"
      },
      "source": [
        "class MyModel:\n",
        "\n",
        "    def __init__(self, target: pd.Series, features: pd.DataFrame):\n",
        "        # YOUR CODE HERE\n",
        "        ...\n",
        "    \n",
        "    def __call__(self, pred_input):\n",
        "        prediction, *_ = self.model.predict([pred_input])\n",
        "        probability, *_ = self.model.predict_proba([pred_input])\n",
        "        confidence = max(probability) * self.test_score\n",
        "        return prediction, confidence\n",
        "    \n",
        "    def __repr__(self):\n",
        "        train = f\"Training Score: {100*self.train_score:.2f}%\"\n",
        "        test = f\"Validation Score: {100*self.test_score:.2f}%\"\n",
        "        return f\"Model(target, features)\\n{train}\\n{test}\"\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.__repr__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL9nZpjgYfHY"
      },
      "source": [
        "Read data from the monster.csv file we created in a previous step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jebKw1LJYRFe"
      },
      "source": [
        "df = pd.read_csv(...)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MlXSvIDeuw1"
      },
      "source": [
        "Drop or encode non-numeric data, except for our rarity target, we'll need that one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DouEcYjUK1xh"
      },
      "source": [
        "df = df.drop(...)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjY2YKbcezEg"
      },
      "source": [
        "### Set Target & Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFnIsGJhfQE2"
      },
      "source": [
        "Target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vANRWmaye29H"
      },
      "source": [
        "target = df[\"Rarity\"]\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvrbdCKJfPpW"
      },
      "source": [
        "Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0B-ZSGDTfPNa"
      },
      "source": [
        "features = df.drop(columns=[\"Rarity\"])\n",
        "features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr19j4gLHUCX"
      },
      "source": [
        "### Model Scoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn-U_pugBm30"
      },
      "source": [
        "Train the model on target and features defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_WBfrQgSlwC"
      },
      "source": [
        "model = Model(...)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yFHej7X3nor"
      },
      "source": [
        "model.model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BI1Da9sMMPvj"
      },
      "source": [
        "### Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kNF_ZtTH6wg"
      },
      "source": [
        "def prediction(pred_input, model):\n",
        "    pred, conf = model([*pred_input])\n",
        "    return f\"Prediction: {pred}\", f\"Confidence: {100*conf:.0f}%\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPdDZhQnCaSB"
      },
      "source": [
        "test_case = {\n",
        "    \"level\": 1,\n",
        "    \"health\": 2,\n",
        "    \"energy\": 2,\n",
        "    \"sanity\": 2, \n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0B2lbsAngmDz"
      },
      "source": [
        "### Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kb2mj_PDHWg"
      },
      "source": [
        "pred, confidence = prediction(list(test_case.values()), model)\n",
        "\n",
        "print(pred)\n",
        "print(confidence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUjNm3jVuHZh"
      },
      "source": [
        "Make some more test cases and make some predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ymWjr5zuLW2"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlPDYEtA59GC"
      },
      "source": [
        "# Assignment 2 - Pipelines & Tuning Review"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc1fCqjv4O1_"
      },
      "source": [
        "## Hyper-parameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxXwrH_0idBi"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFp9WeSH0auo"
      },
      "source": [
        "features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBtSCTxw4j9x"
      },
      "source": [
        "target.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agximo73h3ri"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features,\n",
        "    target,\n",
        "    test_size=0.20,\n",
        "    stratify=target,\n",
        "    random_state=42,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_IKqbDWiG_T"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNhz95wqiDYn"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-xyqk-CxSIn"
      },
      "source": [
        "Calculate the Population Target Class Weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwP35QRUq2P_"
      },
      "source": [
        "total = target.shape[0]\n",
        "counts = target.value_counts()\n",
        "\n",
        "# Flat weights are like a baseline for comparison\n",
        "class_weight_flat = dict(zip(counts.index, [0.1666] * len(counts)))\n",
        "class_weight = dict(zip(counts.index, map(lambda x: x / total, counts)))\n",
        "\n",
        "print(class_weight_flat, class_weight, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPknLxskxygr"
      },
      "source": [
        "Compare the two class weight strategies by completing the code below. Which is better - flat weights or the custom class weights, and why?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoqatVB8CSLq"
      },
      "source": [
        "param_dist = {\n",
        "    \"bootstrap\": (True, False),\n",
        "    \"criterion\": (\"gini\", \"entropy\"),\n",
        "    \"max_depth\": (None, 3, 6, 9),\n",
        "    \"class_weight\": (...),\n",
        "}\n",
        "\n",
        "n_iter = 1\n",
        "for arr in param_dist.values():\n",
        "    n_iter *= len(arr)\n",
        "n_iter\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    RandomForestClassifier(\n",
        "        class_weight=class_weight,\n",
        "        n_estimators=333,\n",
        "        random_state=42,\n",
        "    ),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=n_iter,\n",
        "    n_jobs=-1,\n",
        "    cv=7,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C9iCI3gCQxG"
      },
      "source": [
        "def validation_accuracy(model):\n",
        "    validation_set = [Monster().to_dict() for _ in range(1000)]\n",
        "    df = pd.DataFrame(validation_set)\n",
        "    df = df.drop(columns=[\"Name\", \"Damage\", \"Type\", \"Time Stamp\"])\n",
        "    y_val = df['Rarity']\n",
        "    x_val = df.drop(columns=['Rarity'])\n",
        "    naive_baseline = 1 / len(y_val.unique())\n",
        "    weighted_baseline = max(class_weight.values())\n",
        "    print(f\"Naive Baseline: {100 * naive_baseline:.2f}%\")\n",
        "    print(f\"Weighted Baseline: {100 * weighted_baseline:.2f}%\")\n",
        "    print(f\"Test Accuracy: {100 * model.score(X_test, y_test):.2f}%\")\n",
        "    print(f\"Validation Accuracy: {100 * model.score(x_val, y_val):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOmMUbfK_o_f"
      },
      "source": [
        "validation_accuracy(search)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mq8att2k4TfX"
      },
      "source": [
        "## Machine Learning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiVSIKpA4fmc"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipeline = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    RandomForestClassifier(\n",
        "        class_weight=class_weight,\n",
        "        n_estimators=333,\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    ),\n",
        ").fit(features, target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDrXmLaQ6elj"
      },
      "source": [
        "# Assignment 3 - Clustering Model & Lookup Table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_YlqSAFB542"
      },
      "source": [
        "## Custom Lookup Table - Synthetic Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs459B9ACR8S"
      },
      "source": [
        "A lookup table is better than machine learning if you have **all the data**. \n",
        "\n",
        "Here we do not have all the data, we'll be using the lookup table to design a hard-mode test rather than make predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvYdXicaEZ-A"
      },
      "source": [
        "Systematic Domain Coverage with less-random data. It's still random, but not as random as before, beacuse we're specifying exact levels and rarity ranks. In fact, we have every possible combination of those two features. This is good domain coverage but not complete. Something like this is not suitable for making predictions, but it can be useful for designing a hard-mode validation test.\n",
        "\n",
        "A test based on this lookup table is hard-mode because our data typically follows a non-flat distribution of level and rarity. This table flattens our distribution - we'll have exactly one of each combination of level and rarity. Even though, in the wild we would need a long-long time to naturally get at least one of each combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrqHHZr3CHNG"
      },
      "source": [
        "ranks = [\n",
        "    \"Rank 0\",\n",
        "    \"Rank 1\",\n",
        "    \"Rank 2\",\n",
        "    \"Rank 3\",\n",
        "    \"Rank 4\",\n",
        "    \"Rank 5\",\n",
        "]\n",
        "\n",
        "levels = range(1, 21)\n",
        "\n",
        "monsters = [\n",
        "    Monster(level=level, rarity=rank).to_dict() \n",
        "    for rank in ranks \n",
        "    for level in levels\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgZGkKvkEe3c"
      },
      "source": [
        "A Lookup Table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyxGyKsXBZrY"
      },
      "source": [
        "level = 1\n",
        "rarity = \"Rank 0\"\n",
        "\n",
        "df_lookup = pd.DataFrame(monsters)\n",
        "targets = df_lookup[\n",
        "    (df_lookup[\"Level\"] == level) & (df_lookup[\"Rarity\"] == rarity)\n",
        "].to_dict(orient=\"records\")\n",
        "targets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE9etllvvIsA"
      },
      "source": [
        "Parameterize the above code such that you define an interface for the core logic with inputs for level and rarity, it should return a target list of best matches. You can use a class or function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Awajv3MBvPh-"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2M7FhWsu_6X"
      },
      "source": [
        "Prediction Test: Hard Mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM9YFMHsDak2"
      },
      "source": [
        "def pred_test(target, model):\n",
        "    pred, conf = model([\n",
        "        target[\"Level\"], \n",
        "        target[\"Health\"], \n",
        "        target[\"Energy\"], \n",
        "        target[\"Sanity\"],\n",
        "    ])\n",
        "    keys = [\"Actual\", \"Prediction\", \"Confidence\", \"Correct\"]\n",
        "    values = [rarity, pred, conf, pred == target[\"Rarity\"]]\n",
        "    return dict(zip(keys, values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woxzFYs09RNS"
      },
      "source": [
        "Why is this prediction test \"hard mode\"?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IazxIFCQLG4H"
      },
      "source": [
        "df = pd.DataFrame(pred_test(target, model) for target in monsters)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWatFBWmwr_I"
      },
      "source": [
        "Get the average of the \"Correct\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boLEAAjVNDQ7"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9jXOWRkwzBe"
      },
      "source": [
        "Get the average of the \"Confidence\" column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9rDIv7pCKiw"
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZlnURy3w4qO"
      },
      "source": [
        "What can be said about the correctness average vs. the confidence average. Is this result what you expected and why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ig6ddg6wE2b"
      },
      "source": [
        "## Clustering Model: KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLyEWMBl7D2T"
      },
      "source": [
        "When you don't know what else to do, when you don't even know what the target should be, clustering can help."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dwCxWn_xRNB"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "from typing import Iterable, Iterator, Dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Yl7XLnuPzb"
      },
      "source": [
        "Make 5000 fresh data points with a list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0A4o27k4sg-"
      },
      "source": [
        "# Cluster Lookup Data\n",
        "monsters = [...]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB--4qUQukTW"
      },
      "source": [
        "Drop or encode non-numeric values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGOFyQlxylzT"
      },
      "source": [
        "# Cluster Training Data\n",
        "df = pd.DataFrame(monsters).drop(columns=[\"Name\", \"Damage\", \"Type\", \"Time Stamp\", \"Rarity\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POLCvozq6zAF"
      },
      "source": [
        "Comlete the code below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9QQ8WMS8HjV"
      },
      "source": [
        "class ClusterModel:\n",
        "\n",
        "    def __init__(self, \n",
        "                 lookup_data: Iterable[Dict], \n",
        "                 training_data: pd.DataFrame, \n",
        "                 n_neighbors: int):\n",
        "        self.lookup = lookup_data\n",
        "        self.knn = NearestNeighbors(...)\n",
        "        self.knn.fit(...)\n",
        "\n",
        "    def __call__(self, inputs: Iterable[int]) -> Iterator[Dict]:\n",
        "        nearest = self.knn.kneighbors([inputs], return_distance=False)[0]\n",
        "        return map(lambda n: self.lookup[n], nearest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HjzsOdG9r79"
      },
      "source": [
        "cluster = ClusterModel(\n",
        "    lookup_data=..., \n",
        "    training_data=..., \n",
        "    n_neighbors=...,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o68ATlkx-DIR"
      },
      "source": [
        "for monster in cluster([2, 3, 3, 3]):\n",
        "    print(monster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R42inYnUHY6l"
      },
      "source": [
        "# Assignment 4 - Model Serialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czQ6HT-n80mG"
      },
      "source": [
        "Pickle is another option for serialization, but it is recommended to use joblib when building production apps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7VLM9B7wZZZ"
      },
      "source": [
        "from joblib import dump, load"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1ONoA9svo6s"
      },
      "source": [
        "Save the `model.job` with `dump()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3H09m2RGo1J"
      },
      "source": [
        "...(model, \"model.job\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cFXNWeIu5zH"
      },
      "source": [
        "Open the `model.job` with `load()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK_g_oMSRp7M"
      },
      "source": [
        "saved_model = ...(\"model.job\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UonnIoL79zfO"
      },
      "source": [
        "## Bonus Round - The Machine\n",
        "\n",
        "If assignment 4 took less than 10 minutes, you should do the Bonus Round."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcAqvTNmyfqI"
      },
      "source": [
        "Implement the model interface below. Use your favorite clasification model from any library.\n",
        "\n",
        "The Machine should take a `target` pd.Series and a `features` pd.DataFrame as input. It should then do a train/teast split. Then define the model you want to use and fit it with the training set. See the Model interface in assignment 1 for inspiration.\n",
        "\n",
        "This interface will serve as an [Abstraction Layer](https://en.wikipedia.org/wiki/Abstraction_layer) for your model. Abstraction layers are one of the most overlooked and under valued constructs in all of programming. In this assignment, we will [encapsulate](https://en.wikipedia.org/wiki/Encapsulation_(computer_programming)) or abstract away the type of model we're using by creating a interface class. This interface could be replaced by another one that wraps a different type of model. As long as the same methods with the same signatures are on both interfaces, the rest of the app won't even know. The polymorphic abstraction layer gives us this ability, without the rest of the app being reworked, because all calls to the model travel through the same interface.\n",
        "\n",
        "Objects that can replace eachother like this are said to be [Polymorphic](https://en.wikipedia.org/wiki/Polymorphism_(computer_science))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIROOih6USBQ"
      },
      "source": [
        "class Machine:\n",
        "\n",
        "    def __init__(self, target: pd.Series, features: pd.DataFrame):\n",
        "        ...\n",
        "\n",
        "    def __call__(self, features) -> Tuple[str, float]:\n",
        "        ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgDn-l9dWIqW"
      },
      "source": [
        "custom_machine = Machine(target, features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qaye1l4szbOv"
      },
      "source": [
        "dump(custom_machine, \"machine.job\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9192Pm5aICmZ"
      },
      "source": [
        "machine = load(\"machine.job\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srSx84PHV1-v"
      },
      "source": [
        "total = target.shape[0]\n",
        "counts = target.value_counts()\n",
        "class_weight = dict(zip(counts.index, map(lambda x: x / total, counts)))\n",
        "\n",
        "def validation_accuracy(custom_model):\n",
        "    validation_set = [Monster().to_dict() for _ in range(1000)]\n",
        "    df = pd.DataFrame(validation_set)\n",
        "    df = df.drop(columns=[\"Name\", \"Damage\", \"Type\", \"Time Stamp\"])\n",
        "    y_val = df['Rarity']\n",
        "    x_val = df.drop(columns=['Rarity'])\n",
        "    naive_baseline = 1 / len(y_val.unique())\n",
        "    weighted_baseline = max(class_weight.values())\n",
        "    print(f\"Naive Baseline: {100 * naive_baseline:.2f}%\")\n",
        "    print(f\"Weighted Baseline: {100 * weighted_baseline:.2f}%\")\n",
        "    print(f\"Test Accuracy: {100 * custom_model.test_score:.2f}%\")\n",
        "    print(f\"Validation Accuracy: {100 * custom_model.model.score(x_val, y_val):.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NerysUjCxsB"
      },
      "source": [
        "validation_accuracy(machine)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24tPmV49AsOD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}